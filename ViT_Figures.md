# Vision Transformer: Visual Explanations

## Figure 1: Vision Transformer Architecture Overview

```
┌───────────────────────────────────────────────────────────────────┐
│                     Vision Transformer (ViT)                       │
└───────────────────────────────────────────────────────────────────┘
                                  ↓
┌───────────────┐ ┌─────────────────────────┐ ┌────────────────────┐
│  Image (3×H×W) │→│   Split into patches    │→│  Patch Embedding   │
└───────────────┘ └─────────────────────────┘ └────────────────────┘
                                  ↓
┌───────────────┐ ┌─────────────────────────┐
│  CLS Token    │→│   + Position Embedding   │
└───────────────┘ └─────────────────────────┘
                                  ↓
┌───────────────────────────────────────────────────────────────────┐
│                      Transformer Encoder (×N)                      │
│  ┌─────────────────┐  ┌────────────────────┐  ┌─────────────────┐ │
│  │ Multi-Head      │  │ Layer Normalization │  │ Feed-Forward    │ │
│  │ Self-Attention  │→ │ + Residual         │→ │ Network         │ │
│  └─────────────────┘  └────────────────────┘  └─────────────────┘ │
└───────────────────────────────────────────────────────────────────┘
                                  ↓
┌───────────────────────────────────────────────────────────────────┐
│                      Classification Head                           │
└───────────────────────────────────────────────────────────────────┘
                                  ↓
┌───────────────────────────────────────────────────────────────────┐
│                      Class Probabilities                           │
└───────────────────────────────────────────────────────────────────┘
```

## Figure 2: Image Patching Process

```
Original Image (144×144×3)
┌────────────────────────┐
│                        │
│                        │
│                        │
│                        │
│                        │
│                        │
└────────────────────────┘
             ↓
     Split into 4×4 patches
┌─────┬─────┬─────┬─────┐
│     │     │     │     │
├─────┼─────┼─────┼─────┤
│     │     │     │     │
├─────┼─────┼─────┼─────┤
│     │     │     │     │
├─────┼─────┼─────┼─────┤
│     │     │     │     │
└─────┴─────┴─────┴─────┘
             ↓
    Flatten each patch
┌───────────────────────┐
│ Patch 1 (4×4×3=48)    │
├───────────────────────┤
│ Patch 2 (4×4×3=48)    │
├───────────────────────┤
│ ...                   │
├───────────────────────┤
│ Patch 36 (4×4×3=48)   │
└───────────────────────┘
             ↓
   Linear projection to emb_dim
┌───────────────────────┐
│ Embedding 1 (dim=32)  │
├───────────────────────┤
│ Embedding 2 (dim=32)  │
├───────────────────────┤
│ ...                   │
├───────────────────────┤
│ Embedding 36 (dim=32) │
└───────────────────────┘
```

## Figure 3: Attention Mechanism

```
                   Multi-Head Attention
┌─────────────────────────────────────────────────────────┐
│                                                         │
│   ┌───────────┐   ┌───────────┐   ┌───────────┐        │
│   │ Head 1    │   │ Head 2    │   │ Head N    │        │
│   └───────────┘   └───────────┘   └───────────┘        │
│         ↑               ↑               ↑              │
│    ┌────┴────┐     ┌────┴────┐     ┌────┴────┐         │
│    │         │     │         │     │         │         │
│ ┌──┴──┐ ┌────┴───┐ ┌┴──┐ ┌───┴────┐ ┌┴──┐ ┌──┴─────┐   │
│ │ Q1  │ │ K1    │ │Q2 │ │ K2    │ │QN │ │ KN    │   │
│ └─────┘ └────────┘ └───┘ └────────┘ └───┘ └────────┘   │
│    ↑        ↑        ↑       ↑        ↑       ↑        │
└────┼────────┼────────┼───────┼────────┼───────┼────────┘
     │        │        │       │        │       │
┌────┴────────┴────────┴───────┴────────┴───────┴────────┐
│                     Input Embeddings                    │
└─────────────────────────────────────────────────────────┘
```

## Figure 4: Training Process

```
┌─────────────────────────────────────────────────────────┐
│                                                         │
│                  Oxford-IIIT Pet Dataset                │
│                    (37 Pet Categories)                  │
│                                                         │
└────────────────────────────┬────────────────────────────┘
                             │
          ┌─────────────────┴─────────────────┐
          │                                   │
┌─────────▼────────────┐         ┌────────────▼─────────┐
│                      │         │                      │
│     Training Set     │         │     Testing Set      │
│       (80%)          │         │       (20%)          │
│                      │         │                      │
└──────────┬───────────┘         └──────────┬───────────┘
           │                                │
           │                                │
┌──────────▼───────────┐         ┌──────────▼───────────┐
│                      │         │                      │
│    Data Batching     │         │   Model Evaluation   │
│    (Batch Size: 32)  │         │                      │
│                      │         │                      │
└──────────┬───────────┘         └──────────────────────┘
           │
┌──────────▼───────────┐
│                      │
│  Optimizer: AdamW    │
│  Learning Rate: 0.001│
│  Loss: CrossEntropy  │
│                      │
└──────────────────────┘
```

## Figure 5: Model Performance

```
Training and Testing Loss
    │
Loss│
    │         Training Loss
    │             ●
    │              ●
    │               ●
    │                ●
    │                 ●
    │                  ●    Testing Loss
    │                   ●    ■
    │                    ●    ■
    │                     ●    ■
    │                      ●    ■
    │                       ●    ■
    │                        ●●●  ■■
    │                           ●●●■■■
    └───────────────────────────────────────►
                     Epochs
``` 